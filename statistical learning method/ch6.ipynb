{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlogistic regression\\nmaximum entropy IIS\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "logistic regression\n",
    "maximum entropy IIS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "X_train =np.array([[0.1,1.1],[6.8,7.1],[-3.5,-4.1],[2.0,2.7],[4.1,2.8],[3.1,5.0],[-0.8,-1.3],[0.9,1.2],[5.0,6.4],[3.9,4.0],\n",
    "                  [7.1,4.2],[-1.4,-4.3],[4.5,0.0],[6.3,1.6],[4.2,1.9],[1.4,-3.2],[2.4,-4.0],[2.5,-6.1],[8.4,3.7],[4.1,-2.2]],\n",
    "                 dtype='float')\n",
    "y_train =np.array([1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0],dtype='float')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "y_train =y_train[:,np.newaxis]\n",
    "print(y_train.shape)\n",
    "data =np.concatenate([X_train,y_train],axis=1)\n",
    "print(data[1,:-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+(np.exp(-x)))\n",
    "def crossentropy(t,y):\n",
    "    [n,p]=t.shape\n",
    "    loss =0;\n",
    "    for i in range(n):\n",
    "        loss= loss +t[i]*(np.log(y[i]+1e-7))+(1-t[i])*(np.log(1-y[i]+1e-7))\n",
    "    return -1*loss\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    '''\n",
    "    f(y|x) =1/(1+exp(-1*alpha))\n",
    "    '''\n",
    "    def __init__(self,beta=None):\n",
    "        self.beta =beta\n",
    "    def train(self,data,method='grad'):\n",
    "        '''\n",
    "        method: grad gradient descent\n",
    "                iris iteratiove reweighted least squares\n",
    "                dfp\n",
    "                bfgs\n",
    "        '''\n",
    "        [n,p]=data.shape\n",
    "        p =p-1\n",
    "        X =data[:,0:p].copy()\n",
    "        X =np.concatenate([X,np.ones((n,1),dtype=float)],axis=1)\n",
    "        t =data[:,-1].copy()\n",
    "        t =t[:,np.newaxis]\n",
    "        y =np.zeros((n,1),dtype=float)\n",
    "        beta =0.5*np.ones((p+1,1),dtype=float)\n",
    "        beta_old =np.ones((p+1,1),dtype=float)\n",
    "        hessian =np.zeros((p+1,p+1),dtype=float)\n",
    "        eps =0.001\n",
    "        if(method=='irls'):\n",
    "            while(np.linalg.norm((beta_old -beta))>eps and np.linalg.norm(beta)>eps):\n",
    "                \n",
    "                beta_old =beta\n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta) for x in X ])),dtype=float)\n",
    "                RR =np.multiply(y,1-y)+1e-9   #numerical stability\n",
    "                R =np.diag((RR.flatten()))\n",
    "                Z =X@beta -np.linalg.inv(R)@(y-t)\n",
    "                hessian =X.T@R@X\n",
    "                beta =np.linalg.inv(hessian)@(X.T@R)@Z\n",
    "            self.beta =beta\n",
    "            return beta\n",
    "        elif(method=='grad'):\n",
    "            while(np.linalg.norm((beta_old-beta))>eps and  np.linalg.norm(beta)>eps):\n",
    "                beta_old =beta\n",
    "                y=np.array(list(map(sigmoid,[x.dot(beta) for x in X])),dtype=float)\n",
    "                grad =X.T@(y-t)\n",
    "                beta =beta -grad\n",
    "            self.beta=beta\n",
    "            return beta\n",
    "        elif(method=='dfp'):\n",
    "            G =np.eye(p+1)\n",
    "            grad_old =np.ones((p+1,1),dtype=float)\n",
    "            grad =np.ones((p+1,1),dtype=float)\n",
    "            while(np.linalg.norm(beta_old -beta)>eps  and np.linalg.norm(beta)>eps):\n",
    "                #print('old ',beta_old,'\\n','current ',beta)\n",
    "                #print('grad ',grad)\n",
    "                grad_old =grad\n",
    "                beta_old =beta\n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta) for x in X])),dtype=float)\n",
    "                p =-G@grad_old\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                #0.618 linear search  \n",
    "                # single valley interval\n",
    "                \n",
    "                low =0\n",
    "                high=0.1\n",
    "                gamma =1.1\n",
    "                low_test =0\n",
    "                high_test =0\n",
    "                \n",
    "                low_val =crossentropy(t,y)\n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta+high*p) for x in X])),dtype=float)\n",
    "                high_val =crossentropy(t,y)\n",
    "                while(high_val<=low_val):\n",
    "                    high =high*gamma\n",
    "                    y =np.array(list(map(sigmoid,[x.dot(beta+high*p) for x in X])),dtype=float)\n",
    "                    high_val =crossentropy(t,y)\n",
    "                    print('seeking valley ',' low_val ',low_val,' high_val ',high_val)\n",
    "                    \n",
    "               \n",
    "                c =0.5*(np.sqrt(5)-1)\n",
    "                print(' low ',low,' high ',high)\n",
    "                print('top: low_val ',low_val,'high_val ',high_val)\n",
    "                low_test= low + (high-low)*(1.0-c)\n",
    "                high_test= low + (high-low)*c\n",
    "                low_prime =low\n",
    "                high_prime =high\n",
    "                #search for minnimal\n",
    "                while(((high-low)/(high_prime-low_prime))>=0.001):\n",
    "                    y_low =np.array(list(map(sigmoid,[x.dot(beta+low_test*p) for x in X])),dtype=float)\n",
    "                    y_high =np.array(list(map(sigmoid,[x.dot(beta+high_test*p) for x in X])),dtype=float)\n",
    "                    low_val =crossentropy(t,y_low)\n",
    "                    high_val =crossentropy(t,y_high)\n",
    "                    print('in while: low_val ',low_val,'high_val ',high_val)\n",
    "                    if(low_val<high_val):\n",
    "                        high =high_test\n",
    "                        low_test =low +(1.0-c)*(high-low)\n",
    "                        high_test =low +(high-low)*c\n",
    "                    else:\n",
    "                        low =low_test\n",
    "                        low_test =low+(high-low)*(1.0 - c)\n",
    "                        high_test =low +(high-low)*c\n",
    "                print('p ',p)\n",
    "                print('$\\lambda$ ',0.5*(low+high))\n",
    "                beta =beta+0.5*(low+high)*p\n",
    "                \n",
    "                \"\"\"\n",
    "                \n",
    "                alpha =0.25\n",
    "                f =0.8\n",
    "                step =1\n",
    "                prime_val =crossentropy(t,y)\n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta+step*p) for x in X])),dtype=float)\n",
    "                search_val =crossentropy(t,y)\n",
    "                while(search_val>(prime_val+alpha*step*(grad.T@p))):\n",
    "                    step =f*step\n",
    "                    y =np.array(list(map(sigmoid,[x.dot(beta+step*p) for x in X])),dtype=float)\n",
    "                    search_val =crossentropy(t,y)    \n",
    "                beta =beta +step*p\n",
    "                \n",
    "    \n",
    "    \n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta) for x in X])),dtype=float)\n",
    "                grad =X.T@(y-t)\n",
    "                #theta_k =0.5*(low+high)*p\n",
    "                theta_k =step*p\n",
    "                y_k =grad -grad_old\n",
    "                P_k =(theta_k@theta_k.T)/(theta_k.T@y_k)\n",
    "                Q_k =-1*(G@y_k@y_k.T@G)/(y_k.T@G@y_k)\n",
    "                G =G+P_k+Q_k\n",
    "            self.beta=beta\n",
    "            return beta\n",
    "        elif(method=='bfgs'):\n",
    "            B =np.eye(p+1)\n",
    "            grad_old =np.ones((p+1,1),dtype=float)\n",
    "            grad =np.ones((p+1,1),dtype=float)\n",
    "            while(np.linalg.norm(beta_old -beta)>eps  and np.linalg.norm(beta)>eps):\n",
    "                #print('old ',beta_old,'\\n','current ',beta)\n",
    "                #print('grad ',grad)\n",
    "                grad_old =grad\n",
    "                beta_old =beta\n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta) for x in X])),dtype=float)\n",
    "                p =-np.linalg.inv(B)@grad_old\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                #0.618 linear search  \n",
    "                # single valley interval\n",
    "                \n",
    "                low =0\n",
    "                high=0.1\n",
    "                gamma =1.1\n",
    "                low_test =0\n",
    "                high_test =0\n",
    "                \n",
    "                low_val =crossentropy(t,y)\n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta+high*p) for x in X])),dtype=float)\n",
    "                high_val =crossentropy(t,y)\n",
    "                while(high_val<=low_val):\n",
    "                    high =high*gamma\n",
    "                    y =np.array(list(map(sigmoid,[x.dot(beta+high*p) for x in X])),dtype=float)\n",
    "                    high_val =crossentropy(t,y)\n",
    "                    print('seeking valley ',' low_val ',low_val,' high_val ',high_val)\n",
    "                    \n",
    "               \n",
    "                c =0.5*(np.sqrt(5)-1)\n",
    "                print(' low ',low,' high ',high)\n",
    "                print('top: low_val ',low_val,'high_val ',high_val)\n",
    "                low_test= low + (high-low)*(1.0-c)\n",
    "                high_test= low + (high-low)*c\n",
    "                low_prime =low\n",
    "                high_prime =high\n",
    "                #search for minnimal\n",
    "                while(((high-low)/(high_prime-low_prime))>=0.001):\n",
    "                    y_low =np.array(list(map(sigmoid,[x.dot(beta+low_test*p) for x in X])),dtype=float)\n",
    "                    y_high =np.array(list(map(sigmoid,[x.dot(beta+high_test*p) for x in X])),dtype=float)\n",
    "                    low_val =crossentropy(t,y_low)\n",
    "                    high_val =crossentropy(t,y_high)\n",
    "                    print('in while: low_val ',low_val,'high_val ',high_val)\n",
    "                    if(low_val<high_val):\n",
    "                        high =high_test\n",
    "                        low_test =low +(1.0-c)*(high-low)\n",
    "                        high_test =low +(high-low)*c\n",
    "                    else:\n",
    "                        low =low_test\n",
    "                        low_test =low+(high-low)*(1.0 - c)\n",
    "                        high_test =low +(high-low)*c\n",
    "                print('p ',p)\n",
    "                print('$\\lambda$ ',0.5*(low+high))\n",
    "                beta =beta+0.5*(low+high)*p\n",
    "                \n",
    "                \"\"\"\n",
    "                \n",
    "                alpha =0.25\n",
    "                f =0.8\n",
    "                step =1\n",
    "                prime_val =crossentropy(t,y)\n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta+step*p) for x in X])),dtype=float)\n",
    "                search_val =crossentropy(t,y)\n",
    "                while(search_val>(prime_val+alpha*step*(grad.T@p))):\n",
    "                    step =f*step\n",
    "                    y =np.array(list(map(sigmoid,[x.dot(beta+step*p) for x in X])),dtype=float)\n",
    "                    search_val =crossentropy(t,y)    \n",
    "                beta =beta +step*p\n",
    "                \n",
    "    \n",
    "    \n",
    "                y =np.array(list(map(sigmoid,[x.dot(beta) for x in X])),dtype=float)\n",
    "                grad =X.T@(y-t)\n",
    "                #theta_k =0.5*(low+high)*p\n",
    "                theta_k =step*p\n",
    "                y_k =grad -grad_old\n",
    "                P_k =(y_k@theta_k.T)/(theta_k.T@theta_k)\n",
    "                Q_k =-1*(B@theta_k@theta_k.T@B)/(theta_k.T@B@theta_k)\n",
    "                B =B+P_k+Q_k\n",
    "            self.beta =beta\n",
    "            return beta\n",
    "        \n",
    "        def predict(self,data):\n",
    "            X=data.copy()\n",
    "\n",
    "            [n,p]=X.shape\n",
    "            X =np.concatenate([X,np.ones((n,1),dtype=float)],axis=1)\n",
    "            p=p+1\n",
    "            y_predict =np.array(list(map(sigmoid,[x.dot(self.beta) for x in X])),dtype=float)\n",
    "            return y_predict\n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =LogisticRegression()\n",
    "beta =lr.train(data,method='bfgs')\n",
    "X =data[:,:-1].copy()\n",
    "t =data[:,-1].copy()\n",
    "X =np.concatenate([X,np.ones((20,1))],axis=1)\n",
    "t =t[:,np.newaxis]\n",
    "y =np.array(list(map(sigmoid,[x.dot(beta) for x in X])),dtype=float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y  [[1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [3.64134374e-033]\n",
      " [1.44087337e-028]\n",
      " [2.58992559e-078]\n",
      " [4.17301028e-085]\n",
      " [3.86909375e-014]\n",
      " [1.51472519e-079]\n",
      " [1.84485467e-132]\n",
      " [1.58421262e-196]\n",
      " [3.05232012e-086]\n",
      " [1.80355137e-130]]\n"
     ]
    }
   ],
   "source": [
    "print('t ',t)\n",
    "print('y ',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad  [[-24.53806799]\n",
      " [ 28.13204894]\n",
      " [ 38.11747126]]\n",
      "irls  [[-2.41849180e+08]\n",
      " [ 2.09407138e+08]\n",
      " [ 4.16091170e+08]]\n",
      "dfp  [[-2.41849180e+08]\n",
      " [ 2.09407138e+08]\n",
      " [ 4.16091170e+08]]\n",
      "bfgs  [[-68.24230896]\n",
      " [ 66.99691812]\n",
      " [128.4403822 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('grad ',lr.train(data,method='grad'))\n",
    "print('irls ',lr.train(data,method='irls'))\n",
    "print('dfp ',lr.train(data,method='irls'))\n",
    "print('bfgs ',lr.train(data,method='bfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
